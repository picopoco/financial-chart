{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kbs\\Anaconda3_64\\lib\\site-packages\\sklearn\\utils\\fixes.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() instead\n  if 'order' in inspect.getargspec(np.copy)[0]:\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "from tensorflow.contrib.rnn import MultiRNNCell, RNNCell, DropoutWrapper, LSTMCell, GRUCell, LSTMStateTuple, MultiRNNCell, DropoutWrapper, LayerNormBasicLSTMCell\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    x, y = [], []\n",
    "    until = len(dataset)-look_back-1\n",
    "    for i in range(until):\n",
    "        x.append(dataset[i:(i+look_back), 0])\n",
    "        y.append(dataset[i + look_back, 0])\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-671863e13d53>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'kaggle_bitcoin_from_2017_04_01.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df = pd.read_csv('kaggle_bitcoin_from_2017_04_01.csv', usecols=[7], engine='python')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4059f91367cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 그냥 NaN value를 drop하면 안될 것 같다. 평균 등을 이용해서 전처리 단계에서 NaN 값을 더 채울 수 있도록 보강하자.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df = df.dropna(thresh=1) # 그냥 NaN value를 drop하면 안될 것 같다. 평균 등을 이용해서 전처리 단계에서 NaN 값을 더 채울 수 있도록 보강하자.\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "dataset = dataset.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: Y는 scale 제외\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.8) # train, validation, test 3개 셋으로 나눠서 오버피팅, 언더피팅 상태를 좀 확인해보면 좋을 것 같다.\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00511307],\n",
       "       [ 0.00517356],\n",
       "       [ 0.00534809],\n",
       "       [ 0.00551021],\n",
       "       [ 0.00633264],\n",
       "       [ 0.0055353 ],\n",
       "       [ 0.00631988],\n",
       "       [ 0.00575346],\n",
       "       [ 0.00524336],\n",
       "       [ 0.00558454]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54128867],\n",
       "       [ 0.5403586 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reshape into X=t and Y=t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "look_back = 5\n",
    "train_x, train_y = create_dataset(train, look_back)\n",
    "test_x, test_y = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00511307,  0.00517356,  0.00534809,  0.00551021,  0.00633264],\n",
       "       [ 0.00517356,  0.00534809,  0.00551021,  0.00633264,  0.0055353 ],\n",
       "       [ 0.00534809,  0.00551021,  0.00633264,  0.0055353 ,  0.00631988],\n",
       "       ..., \n",
       "       [ 0.54168195,  0.54295045,  0.54126304,  0.54312426,  0.54375046],\n",
       "       [ 0.54295045,  0.54126304,  0.54312426,  0.54375046,  0.54427153],\n",
       "       [ 0.54126304,  0.54312426,  0.54375046,  0.54427153,  0.5441249 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Model structure parameters\n",
    "in_size = look_back # Size of input vectors at each time step\n",
    "hidden_size = 64 # Size of hidden state vector\n",
    "num_layers = 1 # Number of hidden layers\n",
    "out_size = 1 # Size of output vectors at each time step\n",
    "\n",
    "learning_rate = 0.001 # Learning rate\n",
    "\n",
    "# Data and train parameters\n",
    "batch_size = 64 # Training batch size\n",
    "time_steps = 50 # (Maximum) number of time steps in each batch\n",
    "num_epochs = 1000\n",
    "display_interval = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder for inputs: shape [batch_size, timesteps, in_size]\n",
    "X = tf.placeholder(tf.float32, shape=[None, None, in_size], name='input_X')\n",
    "\n",
    "# Placeholder for outputs: shape [batch_size, timesteps, in_size]\n",
    "Y = tf.placeholder(tf.float32, shape=[None, None, out_size], name='target_Y')\n",
    "\n",
    "# Placeholder for initial state\n",
    "state_size = num_layers * 2 * hidden_size\n",
    "hidden_init = tf.placeholder(tf.float32, shape=[None, state_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create and fit the LSTM network with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_cells = [LSTMCell(num_units=hidden_size, state_is_tuple=True) for i in range(num_layers)]\n",
    "hidden = MultiRNNCell(hidden_cells, state_is_tuple=True)\n",
    "\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell=hidden,\n",
    "                                     dtype=tf.float32,\n",
    "                                     inputs=X)\n",
    "\n",
    "W = tf.get_variable(name='weights', \n",
    "                    shape=[hidden_size, out_size], \n",
    "                    initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable(name='biases', \n",
    "                    shape=[out_size], \n",
    "                    initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "outputs_reshaped = tf.reshape(outputs, [-1, hidden_size])\n",
    "logits = tf.nn.xw_plus_b(x=outputs_reshaped, weights=W, biases=b, name='logits')\n",
    "\n",
    "batch_time_shape = tf.shape(outputs)\n",
    "outputs_activated = tf.reshape(tensor=tf.nn.softmax(logits), \n",
    "                               shape=[batch_time_shape[0], batch_time_shape[1], out_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_X:0\", shape=(?, ?, 5), dtype=float32)\n",
      "Tensor(\"target_Y:0\", shape=(?, ?, 1), dtype=float32)\n",
      "Tensor(\"rnn/transpose:0\", shape=(?, ?, 64), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(?, 64), dtype=float32)\n",
      "Tensor(\"logits:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "print(outputs)\n",
    "print(outputs_reshaped)\n",
    "\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_batch_flatten = tf.reshape(Y, [-1, out_size])\n",
    "cost = tf.reduce_mean(tf.square(logits - Y_batch_flatten))\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_x = np.zeros([batch_size, time_steps, in_size])\n",
    "batch_y = np.zeros([batch_size, time_steps, out_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 5) (64, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "print(batch_x.shape, batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0055353 ,  0.00631988,  0.00575346, ...,  0.54427153,\n",
       "        0.5441249 ,  0.54252273], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \tcost:  0.0256208\n",
      "epoch:  20 \tcost:  0.00139457\n",
      "epoch:  40 \tcost:  0.00130842\n",
      "epoch:  60 \tcost:  0.000915034\n",
      "epoch:  80 \tcost:  0.000829206\n",
      "epoch:  100 \tcost:  0.000760902\n",
      "epoch:  120 \tcost:  0.000572091\n",
      "epoch:  140 \tcost:  0.000506475\n",
      "epoch:  160 \tcost:  0.00038207\n",
      "epoch:  180 \tcost:  0.000309584\n",
      "epoch:  200 \tcost:  0.000436431\n",
      "epoch:  220 \tcost:  0.000270539\n",
      "epoch:  240 \tcost:  0.000259\n",
      "epoch:  260 \tcost:  0.000269103\n",
      "epoch:  280 \tcost:  0.000216526\n",
      "epoch:  300 \tcost:  0.000153397\n",
      "epoch:  320 \tcost:  0.000208664\n",
      "epoch:  340 \tcost:  0.000135264\n",
      "epoch:  360 \tcost:  0.000133325\n",
      "epoch:  380 \tcost:  0.000107058\n",
      "epoch:  400 \tcost:  7.84301e-05\n",
      "epoch:  420 \tcost:  7.29715e-05\n",
      "epoch:  440 \tcost:  7.2327e-05\n",
      "epoch:  460 \tcost:  5.3754e-05\n",
      "epoch:  480 \tcost:  4.35301e-05\n",
      "epoch:  500 \tcost:  4.36992e-05\n",
      "epoch:  520 \tcost:  3.78462e-05\n",
      "epoch:  540 \tcost:  2.8145e-05\n",
      "epoch:  560 \tcost:  2.43939e-05\n",
      "epoch:  580 \tcost:  2.27302e-05\n",
      "epoch:  600 \tcost:  2.13627e-05\n",
      "epoch:  620 \tcost:  1.86059e-05\n",
      "epoch:  640 \tcost:  1.70597e-05\n",
      "epoch:  660 \tcost:  1.36165e-05\n",
      "epoch:  680 \tcost:  1.22484e-05\n",
      "epoch:  700 \tcost:  1.12374e-05\n",
      "epoch:  720 \tcost:  1.12171e-05\n",
      "epoch:  740 \tcost:  8.81619e-06\n",
      "epoch:  760 \tcost:  7.6096e-06\n",
      "epoch:  780 \tcost:  7.87095e-06\n",
      "epoch:  800 \tcost:  7.68893e-06\n",
      "epoch:  820 \tcost:  7.01488e-06\n",
      "epoch:  840 \tcost:  5.52732e-06\n",
      "epoch:  860 \tcost:  6.19779e-06\n",
      "epoch:  880 \tcost:  5.81527e-06\n",
      "epoch:  900 \tcost:  5.13507e-06\n",
      "epoch:  920 \tcost:  5.16853e-06\n",
      "epoch:  940 \tcost:  4.18998e-06\n",
      "epoch:  960 \tcost:  4.48738e-06\n",
      "epoch:  980 \tcost:  3.29035e-06\n"
     ]
    }
   ],
   "source": [
    "possible_batch_idx = range(train_x.shape[0] - time_steps - 1)\n",
    "\n",
    "# Declare the session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        batch_id = random.sample(population=possible_batch_idx, \n",
    "                                 k=batch_size)\n",
    "\n",
    "        for j in range(time_steps):\n",
    "            idx_X = [k + j for k in batch_id]\n",
    "            idx_Y = [k + j + 1 for k in batch_id]\n",
    "\n",
    "            batch_x[:, j, :] = dataset[idx_X, :]\n",
    "            batch_y[:, j, :] = dataset[idx_Y, :]\n",
    "\n",
    "        init_value = np.zeros((batch_x.shape[0], state_size))\n",
    "        training_cost, _ = sess.run([cost, train_op], feed_dict={X:batch_x, Y:batch_y, hidden_init:init_value})\n",
    "\n",
    "        if i % display_interval == 0:\n",
    "            print(\"epoch: \", i, \"\\tcost: \", training_cost)\n",
    "          \n",
    "        # TODO:model saver\n",
    "#         if i % display_interval == 0:\n",
    "#             print(\"epoch: \", i, \"\\tcost: \", training_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "246px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4.0,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}